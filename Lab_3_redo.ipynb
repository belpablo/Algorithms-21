{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3. Algorithms of unconditional nonlinear optimization. First and second order methods\n",
    "\n",
    "### Generate random numbers $\\alpha \\in (0, 1)$  and $\\beta \\in (0, 1)$. Furthermore, generate the noisy data {$x_k, y_k$}, where $k = 0, 1, .. 100$, according to the rule: $x_k = \\frac{k}{100}$, ${y_k = \\alpha x_k + \\beta + \\delta_k}$, where $\\delta_k \\sim N(0, 1)$ are values of a random variable with standard normal distribution. Approximate the data by the following linear and rational functions:\n",
    "\n",
    "### $1) F(x, a, b) = ax + b$ (linear approximant)\n",
    "### $2) F(x, a, b) = \\frac{a}{1 + bx}$ (rational approximant)\n",
    "\n",
    "### by means of least squares through the numerical minimization (with precision $\\varepsilon = 0.001$) of the following function:\n",
    "\n",
    "### $D(a, b) = \\sum\\limits_{k = 0}^{100}{(F(x_k, a, b) - y_k)^2}$\n",
    "\n",
    "### To solve the minimization problem, use gradient descent,the conjugate gradient method, the Newton method and the Levenberg-Marquardt algorithm. If necessary, set the initial approximations and other parameters of the methods yourself. \n",
    "\n",
    "### On the graph (separately for each approximating function), draw an array of generateddata and graphs of approximating functions obtained using these numerical optimization algorithms. Analyzethe results obtained in terms of the number of iterations performed. Compare the results obtained with the results from previous task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exhaustive search</th>\n",
       "      <th>coordinate descent method</th>\n",
       "      <th>Nelder Mead method</th>\n",
       "      <th>gradient descent method</th>\n",
       "      <th>conjugate gradient descent method</th>\n",
       "      <th>Newton's method</th>\n",
       "      <th>Levenberg-Marquardt alghorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>a</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.4508</td>\n",
       "      <td>1.2370</td>\n",
       "      <td>0.4508</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>b</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>0.5481</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nfev</td>\n",
       "      <td>7834402.000</td>\n",
       "      <td>1744.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>88.0000</td>\n",
       "      <td>31.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nit</td>\n",
       "      <td>7834402.000</td>\n",
       "      <td>150.000</td>\n",
       "      <td>53.000</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      exhaustive search  coordinate descent method  Nelder Mead method  \\\n",
       "a                 0.451                      0.451               0.451   \n",
       "b                 0.965                      0.964               0.965   \n",
       "nfev        7834402.000                   1744.000             104.000   \n",
       "nit         7834402.000                    150.000              53.000   \n",
       "\n",
       "      gradient descent method  conjugate gradient descent method  \\\n",
       "a                      0.4508                             1.2370   \n",
       "b                      0.9649                             0.5481   \n",
       "nfev                  88.0000                            31.0000   \n",
       "nit                    8.0000                             2.0000   \n",
       "\n",
       "      Newton's method  Levenberg-Marquardt alghorithm  \n",
       "a              0.4508                             1.0  \n",
       "b              0.9649                             1.0  \n",
       "nfev           4.0000                             1.0  \n",
       "nit            3.0000                             1.0  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import misc\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize, minimize_scalar\n",
    "\n",
    "global x_arr\n",
    "global y_arr\n",
    "\n",
    "\n",
    "def least_squares_linear(args):\n",
    "    return np.sum(np.square(args[0] * x_arr + args[1] - y_arr))\n",
    "\n",
    "\n",
    "def least_squares_rational(args):\n",
    "    return np.sum(np.square(args[0] / (1 + args[1] * x_arr) - y_arr))\n",
    "\n",
    "\n",
    "def gradient_scipy(func, args, eps = 1e-5):\n",
    "    \n",
    "    a_arg, b_arg = args[0], args[1]\n",
    "\n",
    "    part_deriv_a = misc.derivative(lambda a: func([a, b_arg]), a_arg, dx = eps)\n",
    "    part_deriv_b = misc.derivative(lambda b: func([a_arg, b]), b_arg, dx = eps)\n",
    "\n",
    "    return np.array(part_deriv_a, part_deriv_b)\n",
    "\n",
    "\n",
    "def gradient_num(func, args, eps = 1e-5):\n",
    "    \n",
    "    a_arg, b_arg = args[0], args[1]\n",
    "    \n",
    "    #n_digits = abs(int(round(np.log10(eps))))\n",
    "    #part_deriv_a = round((func([a_arg + eps, b_arg]) - func([a_arg - eps, b_arg])) / (2 * eps), n_digits)\n",
    "    #part_deriv_b = round((func([a_arg, b_arg + eps]) - func([a_arg, b_arg - eps])) / (2 * eps), n_digits)\n",
    "    \n",
    "    part_deriv_a = (func([a_arg + eps, b_arg]) - func([a_arg - eps, b_arg])) / (2 * eps)\n",
    "    part_deriv_b = (func([a_arg, b_arg + eps]) - func([a_arg, b_arg - eps])) / (2 * eps)\n",
    "\n",
    "    return np.array([part_deriv_a,  part_deriv_b])\n",
    "\n",
    "\n",
    "def hessian_num(func, args, eps = 1e-5):                            # value of hessian of func at the point args = [a, b]\n",
    "    \n",
    "    a_arg, b_arg = args[0], args[1]\n",
    "    \n",
    "    func_ab = func([a_arg, b_arg])\n",
    "    func_a_eps_b = func([a_arg + eps, b_arg])\n",
    "    func_a_b_eps = func([a_arg, b_arg + eps])\n",
    "    \n",
    "    #n_digits = abs(int(round(np.log10(eps))))\n",
    "    #part_deriv_aa = round((func_a_eps_b - 2 * func_ab + func([a_arg - eps, b_arg])) / (eps ** 2), n_digits)\n",
    "    #part_deriv_bb = round((func_a_b_eps - 2 * func_ab + func([a_arg, b_arg - eps])) / (eps ** 2), n_digits)\n",
    "    #part_deriv_ab = round((func([a_arg + eps, b_arg + eps]) - func_a_eps_b - func_a_b_eps + func_ab) / (eps ** 2), n_digits)\n",
    "\n",
    "    part_deriv_aa = (func_a_eps_b - 2 * func_ab + func([a_arg - eps, b_arg])) / (eps ** 2)\n",
    "    part_deriv_bb = (func_a_b_eps - 2 * func_ab + func([a_arg, b_arg - eps])) / (eps ** 2)\n",
    "    part_deriv_ab = (func([a_arg + eps, b_arg + eps]) - func_a_eps_b - func_a_b_eps + func_ab) / (eps ** 2)\n",
    "    \n",
    "    return np.array([[part_deriv_aa, part_deriv_ab], [part_deriv_ab, part_deriv_bb]])\n",
    "\n",
    "\n",
    "def gradient_descent(func, initial_approx = [1, 0], eps = 1e-4):\n",
    "    \n",
    "    func_eval_num, iter_num = 0, 0\n",
    "    args_prev = np.array(initial_approx)\n",
    "    \n",
    "    args_curr = args_prev - np.dot(gradient_num(func, args_prev), 0.01)\n",
    "    #grad_func_curr = gradient_num(func, args_curr)\n",
    "    \n",
    "    func_eval_num += 4\n",
    "    iter_num += 1\n",
    "    \n",
    "    while (np.linalg.norm(args_curr - args_prev, np.inf) >= eps) and (iter_num < 1000):\n",
    "        \n",
    "        #grad_func_prev = grad_func_curr[:]\n",
    "        grad_func_prev = gradient_num(func, args_prev)\n",
    "        grad_func_curr = gradient_num(func, args_curr)\n",
    "        \n",
    "        beta = np.dot(args_curr - args_prev, grad_func_curr - grad_func_prev) \\\n",
    "        / np.square(np.linalg.norm(grad_func_curr - grad_func_prev))\n",
    "        \n",
    "        #args_prev = args_curr[:]\n",
    "        args_prev = args_curr\n",
    "        args_curr = args_prev - np.dot(gradient_num(func, args_prev), beta)\n",
    "        \n",
    "        #cond = abs(func(args_curr) - func(args_prev))\n",
    "        #print(args_curr[0], args_curr[1], cond) \n",
    "        #plt.scatter(args_curr[0], args_curr[1])\n",
    "        \n",
    "        func_eval_num +=  12\n",
    "        iter_num += 1\n",
    "    \n",
    "    n_digits = abs(int(round(np.log10(eps))))\n",
    "\n",
    "    return np.array([round(args_curr[0], n_digits), round(args_curr[1], n_digits), int(func_eval_num), int(iter_num)])\n",
    "\n",
    "\n",
    "def conjugate_gradient_descent(func, initial_approx = [1, 0], eps = 1e-4):\n",
    "    \n",
    "    func_eval_num, iter_num = 0, 0\n",
    "\n",
    "    args_prev = np.array(initial_approx)\n",
    "    grad_func_prev = gradient_num(func, args_prev)\n",
    "    res = minimize_scalar(lambda x: func(args_prev - x * grad_func_prev))\n",
    "    \n",
    "    args_curr = args_prev - np.dot(res.x, grad_func_prev)\n",
    "    conj_direction_curr = -1 * grad_func_prev\n",
    "    \n",
    "    #grad_func_curr = gradient_num(func, args_curr)\n",
    "\n",
    "    func_eval_num += res.nfev + 8\n",
    "    iter_num += 1\n",
    "    \n",
    "    while (np.linalg.norm(args_curr - args_prev, np.inf) >= eps) and (iter_num < 1000):\n",
    "            \n",
    "        #grad_func_prev = grad_func_curr[:]\n",
    "        grad_func_prev = gradient_num(func, args_prev)\n",
    "        grad_func_curr = gradient_num(func, args_curr)\n",
    "        \n",
    "        beta = np.dot(grad_func_curr, grad_func_curr) / np.dot(grad_func_prev, grad_func_prev)\n",
    "        #beta = np.dot(grad_func_curr, grad_func_curr - grad_func_prev) / np.dot(grad_func_prev, grad_func_prev)\n",
    "        \n",
    "        conj_direction_prev = conj_direction_curr[:]\n",
    "        conj_direction_curr = np.dot(beta, conj_direction_prev) - grad_func_prev\n",
    "        \n",
    "        res = minimize_scalar(lambda x: func(args_curr + x * conj_direction_curr))\n",
    "        \n",
    "        args_prev = args_curr[:]\n",
    "        args_curr = args_prev - np.dot(res.x, conj_direction_curr)\n",
    "        \n",
    "        #cond = abs(func(args_curr) - func(args_prev))\n",
    "        #print(args_curr[0], args_curr[1], cond) \n",
    "        #plt.scatter(args_curr[0], args_curr[1])\n",
    "        \n",
    "        func_eval_num += res.nfev + 4\n",
    "        iter_num += 1\n",
    "    \n",
    "    n_digits = abs(int(round(np.log10(eps))))\n",
    "\n",
    "    return np.array([round(args_curr[0], n_digits), round(args_curr[1], n_digits), int(func_eval_num), int(iter_num)])\n",
    "\n",
    "\n",
    "def Newtons_method(func, initial_approx = [1, 0], eps = 1e-4):\n",
    "    \n",
    "    res = minimize(func, initial_approx, method = 'Newton-CG', jac = (lambda x: gradient_num(func, x)), \\\n",
    "                   hess = (lambda x: hessian_num(func, x)), options = {'xtol': eps})\n",
    "    n_digits = abs(int(round(np.log10(eps))))\n",
    "\n",
    "    return np.array([round(res.x[0], n_digits), round(res.x[1], n_digits), res.nfev, res.nit])\n",
    "                   \n",
    "                   \n",
    "def Levenberg_Marquardt(func, initial_aprox = [1, 0], eps = 1e-4):\n",
    "    return np.ones(4)\n",
    "\n",
    "\n",
    "# Pushing in DataFrame results from this and previous laboratory works\n",
    "\n",
    "x_arr = np.load('x_arr.npy')\n",
    "y_arr = np.load('y_arr.npy')\n",
    "\n",
    "df_lin = pd.read_csv('Linear_approx_lab_2.csv', index_col = 0)\n",
    "df_rat = pd.read_csv('Rational_approx_lab_2.csv', index_col = 0)\n",
    "\n",
    "algos_dict = {'gradient descent method': gradient_descent, 'conjugate gradient descent method': conjugate_gradient_descent, \\\n",
    "              \"Newton's method\": Newtons_method, 'Levenberg-Marquardt alghorithm': Levenberg_Marquardt}\n",
    "\n",
    "for alg_name in algos_dict.keys():\n",
    "    \n",
    "    df_lin[alg_name] = algos_dict[alg_name](least_squares_linear)\n",
    "    df_rat[alg_name] = algos_dict[alg_name](least_squares_rational)\n",
    "    \n",
    "df_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exhaustive search</th>\n",
       "      <th>coordinate descent method</th>\n",
       "      <th>Nelder Mead method</th>\n",
       "      <th>gradient descent method</th>\n",
       "      <th>conjugate gradient descent method</th>\n",
       "      <th>Newton's method</th>\n",
       "      <th>Levenberg-Marquardt alghorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>a</td>\n",
       "      <td>1.012</td>\n",
       "      <td>1.011</td>\n",
       "      <td>1.012</td>\n",
       "      <td>1.0118</td>\n",
       "      <td>1.1462</td>\n",
       "      <td>1.0118</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>b</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.2848</td>\n",
       "      <td>-0.1025</td>\n",
       "      <td>-0.2848</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nfev</td>\n",
       "      <td>7834402.000</td>\n",
       "      <td>3528.000</td>\n",
       "      <td>129.000</td>\n",
       "      <td>148.0000</td>\n",
       "      <td>64.0000</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nit</td>\n",
       "      <td>7834402.000</td>\n",
       "      <td>229.000</td>\n",
       "      <td>68.000</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      exhaustive search  coordinate descent method  Nelder Mead method  \\\n",
       "a                 1.012                      1.011               1.012   \n",
       "b                -0.285                     -0.285              -0.285   \n",
       "nfev        7834402.000                   3528.000             129.000   \n",
       "nit         7834402.000                    229.000              68.000   \n",
       "\n",
       "      gradient descent method  conjugate gradient descent method  \\\n",
       "a                      1.0118                             1.1462   \n",
       "b                     -0.2848                            -0.1025   \n",
       "nfev                 148.0000                            64.0000   \n",
       "nit                   13.0000                             2.0000   \n",
       "\n",
       "      Newton's method  Levenberg-Marquardt alghorithm  \n",
       "a              1.0118                             1.0  \n",
       "b             -0.2848                             1.0  \n",
       "nfev           9.0000                             1.0  \n",
       "nit            8.0000                             1.0  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4] [1, -1, 1, -1, 1] \n",
      "\n",
      "[1, 2, 3, 4, 5] [0, 1, 2, 3, 4] \n",
      "\n",
      "[2, 3, 4, 5, 6] [1, 2, 3, 4, 5] \n",
      "\n",
      "[3, 4, 5, 6, 7] [2, 3, 4, 5, 6] \n",
      "\n",
      "[4, 5, 6, 7, 8] [3, 4, 5, 6, 7] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [i for i in range(5)]\n",
    "b = [(-1) ** i for i in range(5)]\n",
    "print(a, b, '\\n')\n",
    "\n",
    "for j in range(1, 5):\n",
    "    \n",
    "    b = a[:]\n",
    "    a = [i + j for i in range(5)]\n",
    "    \n",
    "    print(a, b, '\\n')\n",
    "\n",
    "#df_rat\n",
    "\n",
    "np.dot(a, a) == np.sum(np.square(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-38.44961147  26.96584518] [-38.44961147  26.96584518]\n",
      "[ -60.29584519 -139.44961147] [ -60.29584519 -139.44961147]\n"
     ]
    }
   ],
   "source": [
    "def least_squares_linear(args):                                 # two-variable func to minimaize\n",
    "    return np.sum(np.square(args[0] * x_arr + args[1] - y_arr))\n",
    "\n",
    "def least_squares_rational(args):\n",
    "    return np.sum(np.square(args[0] / (1 + args[1] * x_arr) - y_arr))\n",
    "\n",
    "def f(X):\n",
    "    return 9 - X[0] ** 2 - X[1] ** 2\n",
    "\n",
    "def gradient(func, args, eps = 1e-5):\n",
    "    \n",
    "    a_arg, b_arg = args[0], args[1]\n",
    "\n",
    "    part_deriv_a = misc.derivative(lambda a: func([a, b_arg]), a_arg, dx = eps)\n",
    "    part_deriv_b = misc.derivative(lambda b: func([a_arg, b]), b_arg, dx = eps)\n",
    "\n",
    "    return np.array([part_deriv_a, part_deriv_b])\n",
    "\n",
    "def gradient_num(func, args, eps = 1e-5):\n",
    "    \n",
    "    a_arg, b_arg = args[0], args[1]\n",
    "    \n",
    "    #n_digits = abs(int(round(np.log10(eps))))\n",
    "    #part_deriv_a = round((func([a_arg + eps, b_arg]) - func([a_arg - eps, b_arg])) / (2 * eps), n_digits)\n",
    "    #part_deriv_b = round((func([a_arg, b_arg + eps]) - func([a_arg, b_arg - eps])) / (2 * eps), n_digits)\n",
    "    \n",
    "    part_deriv_a = (func([a_arg + eps, b_arg]) - func([a_arg - eps, b_arg])) / (2 * eps)\n",
    "    part_deriv_b = (func([a_arg, b_arg + eps]) - func([a_arg, b_arg - eps])) / (2 * eps)\n",
    "\n",
    "    return np.array([part_deriv_a,  part_deriv_b])\n",
    "\n",
    "\n",
    "x_arr = np.load('x_arr.npy')\n",
    "y_arr = np.load('y_arr.npy')\n",
    "\n",
    "tmp_1 = gradient(least_squares_rational, [1, 0])\n",
    "tmp_2 = gradient_num(least_squares_rational, [1, 0])\n",
    "print(tmp_1, tmp_2)\n",
    "\n",
    "tmp_1 = gradient(least_squares_linear, [1, 0])\n",
    "tmp_2 = gradient_num(least_squares_linear, [1, 0])\n",
    "print(tmp_1, tmp_2)\n",
    "\n",
    "# неужели это ожно и то же???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
